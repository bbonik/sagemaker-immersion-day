{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker basics (all-in-one)\n",
    "**Demontstrating SageMaker basics: training jobs, built-in algorithms (XGBoost), dataset balancing, endpoint deployemnt, batch transform, and Hyper-parameter Optimization.**\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Prepare our Environment](#Prepare-our-Environment)\n",
    "1. [Download and Explore the Data](#Download-and-Explore-the-Data)\n",
    "1. [Transform the Data](#Transform-the-Data)\n",
    "1. [Understand the Algorithm](#Understand-the-Algorithm)\n",
    "1. [Upload the Input Data to S3](#Upload-the-Input-Data-to-S3)\n",
    "1. [Train the Model](#Train-the-Model)\n",
    "1. [Deploy and Evaluate the Model](#Deploy-and-Evaluate-the-Model)\n",
    "1. [Hyperparameter Optimization (HPO)](#Hyperparameter-Optimization-(HPO))\n",
    "1. [Conclusions](#Conclusions)\n",
    "1. [Releasing Cloud Resources](#Releasing-Cloud-Resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This workshop aims to give you an **example of using and tuning a SageMaker built-in algorithm**: Focussing on the **data interfaces** and SageMaker's automatic **Hyperparameter Optimization** (HPO) capabilities.\n",
    "\n",
    "Teaching in-depth data science approaches for tabular data is outside this scope, and we hope you can use this notebook as a starting point to modify for the needs of your future projects.\n",
    "\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Direct marketing**: contacting potential new customers via mail, email, phone call etc. \n",
    "- **Challenge**: A) too many potential customers. B) limited resources of the approacher (time, money etc.).\n",
    "- **Problem: Which are the potential customers with the higher chance of becoming actual customers**? (so as to focus the effort only on them). \n",
    "- **Our setting**: A bank who wants to predict *whether a customer will enroll for a term deposit, after one or more phone calls*.\n",
    "- **Our approach**: Build a ML model to do this prediction, from readily available information e.g. demographics, past interactions etc. (features).\n",
    "- **Our tools**: We will be using the **XGBoost** algorithm implementation by **Amazon SageMaker**, and using SageMaker **Hyperparameter Optimization (HPO)** to improve our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare our Environment\n",
    "\n",
    "We'll need to:\n",
    "\n",
    "- **import** some useful libraries (as in any Python notebook)\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to AWS in general (with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "\n",
    "While `boto3` is the general AWS SDK for Python, `sagemaker` provides some powerful, higher-level interfaces designed specifically for ML workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "import time\n",
    "import os\n",
    "from util.classification_report import generate_classification_report  # helper function for classification reports\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"xgboost-example\"  # Location in the bucket to store our files\n",
    "sgmk_session = sagemaker.Session()\n",
    "sgmk_client = boto_session.client(\"sagemaker\")\n",
    "sgmk_role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using role:', sgmk_role)\n",
    "print('Using bucket:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download and Explore the Data\n",
    "\n",
    "Let's start by downloading the [direct marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository.\n",
    "\n",
    "We can run shell commands from inside Jupyter using the `!` prefix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -P data/ -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/bank-additional.zip\", 'r') as zip_ref:\n",
    "    print(\"Unzipping...\")\n",
    "    zip_ref.extractall(\"data\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./data/bank-additional/bank-additional-full.csv\", sep=\";\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)  # Make sure we can see all of the columns\n",
    "df_data.head()  # show part of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Demographics:*\n",
    "* `age`: Customer's age (numeric)\n",
    "* `job`: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* `marital`: Marital status (categorical: 'married', 'single', ...)\n",
    "* `education`: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "*Past customer events:*\n",
    "* `default`: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* `housing`: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* `loan`: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "*Past direct marketing contacts:*\n",
    "* `contact`: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* `month`: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* `day_of_week`: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* `duration`: Last contact duration, in seconds (numeric). Important note: If duration = 0 then `y` = 'no'.\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign`: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* `pdays`: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* `previous`: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* `poutcome`: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "*External environment factors:*\n",
    "* `emp.var.rate`: Employment variation rate - quarterly indicator (numeric)\n",
    "* `cons.price.idx`: Consumer price index - monthly indicator (numeric)\n",
    "* `cons.conf.idx`: Consumer confidence index - monthly indicator (numeric)\n",
    "* `euribor3m`: Euribor 3 month rate - daily indicator (numeric)\n",
    "* `nr.employed`: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "*Target variable* **(the one we want to eventually predict):**\n",
    "* `y`: Has the client subscribed to a term deposit? (binary: 'yes','no')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transform the Data\n",
    "\n",
    "Cleaning up data is part of nearly every ML project. Several common steps include:\n",
    "\n",
    "* **Handling missing values**: In our case there are no missing values.\n",
    "* **Handling weird/outlier values**: There are some values in the dataset that may require manipulation.\n",
    "* **Converting categorical to numeric**: There are a lot of categorical variables in our dataset. We need to address this.\n",
    "* **Oddly distributed data**: We will be using XGBoost, which is a non-linear method, and is minimally affected by the data distribution.\n",
    "* **Remove unnecessary data**: There are lots of columns representing general economic features that may not be available during inference time.\n",
    "\n",
    "To summarise, we need to A) address some weird values, B) convert the categorical to numeric valriables and C) Remove unnecessary data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many records have the value of \"999\" for `pdays`. It is very likely to be a 'magic' number to represent that *no contact was made before*. Considering that, we will create a new column called \"no_previous_contact\", then grant it value of \"1\" when pdays is 999 and \"0\" otherwise.\n",
    "\n",
    "2. In the `job` column, there are more than one categories for people who don't work e.g., \"student\", \"retired\", and \"unemployed\". It is very likely the decision to enroll or not to a term deposit depends a lot on whether the customer is working or not. A such, we generate a new column to show whether the customer is working based on `job` column.\n",
    "\n",
    "3. We will remove the economic features and `duration` from our data as they would need to be forecasted with high precision to be used as features during inference time.\n",
    "\n",
    "4. We convert categorical variables to numeric using *one hot encoding*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator variable to capture when pdays takes a value of 999\n",
    "df_data[\"no_previous_contact\"] = np.where(df_data[\"pdays\"] == 999, 1, 0)\n",
    "\n",
    "# Indicator for individuals not actively employed\n",
    "df_data[\"not_working\"] = np.where(np.in1d(df_data[\"job\"], [\"student\", \"retired\", \"unemployed\"]), 1, 0)\n",
    "\n",
    "# remove unnecessary data\n",
    "df_model_data = df_data.drop(\n",
    "    [\"duration\", \n",
    "    \"emp.var.rate\", \n",
    "    \"cons.price.idx\", \n",
    "    \"cons.conf.idx\", \n",
    "    \"euribor3m\", \n",
    "    \"nr.employed\"], \n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_model_data = pd.get_dummies(df_model_data)  # Convert categorical variables to sets of indicators\n",
    "\n",
    "# Replace \"y_no\" and \"y_yes\" with a single label column, and bring it to the front:\n",
    "df_model_data = pd.concat(\n",
    "    [\n",
    "        df_model_data[\"y_yes\"].rename(\"y\"),\n",
    "        df_model_data.drop([\"y_no\", \"y_yes\"], axis=1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_model_data.head()  # show part of the new transformed dataframe (which will be used for training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understand the Algorithm (XGBoost)\n",
    "\n",
    "We'll be using SageMaker's [built-in **XGBoost Algorithm**](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html): Benefiting from performance-optimized, pre-implemented functionality like multi-instance parallelization, and support for multiple input formats.\n",
    "\n",
    "In general to use the pre-built algorithms, we'll need to:\n",
    "\n",
    "- Refer to the [Common Parameters docs](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) to see the **high-level configuration** and what features each algorithm has\n",
    "- Refer to the [algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to understand the **detail** of the **data formats** and **(hyper)-parameters** it supports\n",
    "\n",
    "From these docs, we'll understand what data format we need to upload to S3 (next), and how to get the container image URI of the algorithm... which is listed on the Common Parameters page but can also be extracted through the SDK:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Upload the Input Data to S3\n",
    "\n",
    "We know from [the algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost) that SageMaker XGBoost expects data in the **libSVM** or **CSV** formats, with:\n",
    "\n",
    "- The target variable in the first column, and\n",
    "- No header row\n",
    "\n",
    "...So before initializing training, we will:\n",
    "\n",
    "1. Suffle and split the data into **Training (70%)**, **Validation (20%)**, and **Test (10%)** sets\n",
    "2. Save the data in the format the algorithm expects (e.g. CSV)\n",
    "3. Upload the data to S3\n",
    "4. Define the training job input \"channels\" with explicit CSV content type tagging, via the SageMaker SDK [TrainingInput](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.inputs.TrainingInput) class\n",
    "\n",
    "The Training and Validation datasets will be used during the training (and tuning) phase, while the 'holdout' Test set will be used afterwards to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and splitting dataset\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_model_data.sample(frac=1, random_state=1729), \n",
    "    [int(0.7 * len(df_model_data)), int(0.9*len(df_model_data))],\n",
    ") \n",
    "\n",
    "# Create CSV files for Train / Validation / Test\n",
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"data/validation.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV files to S3 for SageMaker training\n",
    "train_uri = sgmk_session.upload_data(\n",
    "    path=\"data/train.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix,\n",
    ")\n",
    "val_uri = sgmk_session.upload_data(\n",
    "    path=\"data/validation.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the data input channels for the training job:\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(train_uri, content_type=\"csv\")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(val_uri, content_type=\"csv\")\n",
    "\n",
    "print(f\"{s3_input_train.config}\\n\\n{s3_input_validation.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train the Model\n",
    "\n",
    "Training a model on SageMaker follows the usual steps with other ML libraries (e.g. SciKit-Learn):\n",
    "1. Initiate a session (we did this up top).\n",
    "2. Instantiate an estimator object for our algorithm (XGBoost).\n",
    "3. Define its hyperparameters.\n",
    "4. Start the training job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating dataset imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = np.count_nonzero(train_data[\"y\"].values==1)\n",
    "negative_examples = np.count_nonzero(train_data[\"y\"].values==0)\n",
    "total_examples = positive_examples + negative_examples\n",
    "ratio_neg_pos = negative_examples/positive_examples\n",
    "ratio_pos_neg = positive_examples/negative_examples\n",
    "\n",
    "print('y=0: ', negative_examples, '(', round((negative_examples*100)/total_examples,2), '%)')\n",
    "print('y=1: ', positive_examples, '(', round((positive_examples*100)/total_examples,2), '%)')\n",
    "print('positive/negative examples ratio:', ratio_pos_neg)\n",
    "print('negative/positive examples ratio:', ratio_neg_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative examples (customers who did not opt for the term deposit) are almost 8 times more than the positive examples (customers who opted for the term deposit). This is a considerable dataset imbalance. If we don't address it, our classifier will learn to predict all new inputs as negative, because they are the majority by far. We will address this imbalance by taking into consideration the **negative/positive examples ratio** when we instantiate our SageMaker estimator.\\\n",
    "This can be done by the `scale_pos_weight` parameter of the XGBoost estimator, which **controls the balance of positive and negative weights**. You can find more information [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html). A typical value to consider: sum(negative cases) / sum(positive cases). In our case, we will set `scale_pos_weight` to the negative/positive examples ratio, to undo this imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small competition!\n",
    "SageMaker's XGBoost includes 38 parameters. You can find more information about them [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html).\n",
    "For simplicity, we choose to experiment only with a few of them.\n",
    "\n",
    "**Please select values for the 4 hyperparameters (by replacing the ?) based on the provided ranges.** Later we will see which model performed best and compare it with the one from the Hyperparameter Optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify algorithm container\n",
    "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.0-1\")\n",
    "print(training_image)\n",
    "\n",
    "# Instantiate an XGBoost estimator object\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,      # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,              # number of instances to be used\n",
    "    role=sgmk_role,                # IAM role to be used\n",
    "    max_run=20*60,                 # Maximum allowed active runtime\n",
    "    use_spot_instances=True,       # Use spot instances to reduce cost\n",
    "    max_wait=30*60,                # Maximum clock time (including spot delays)\n",
    ")\n",
    "\n",
    "# scale_pos_weight controls the balance of positive and negative weights. \n",
    "# It's useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). \n",
    "\n",
    "# define its hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    num_round=150,     # int: [1,300]\n",
    "    max_depth=5,     # int: [1,10]\n",
    "    alpha=2,         # float: [0,5]\n",
    "    eta=0.2,           # float: [0,1]\n",
    "    objective=\"binary:logistic\",\n",
    "    scale_pos_weight=ratio_neg_pos,  # set the balance between positive and negative classes, to undo the imbalance\n",
    ")\n",
    "\n",
    "# start a training (fitting) job\n",
    "estimator.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deploy and Evaluate the Model\n",
    "\n",
    "### Deployment\n",
    "\n",
    "Now that we've trained the xgboost algorithm on our data, deploying the model (hosting it behind a real-time endpoint) is just one function call!\n",
    "\n",
    "This deployment might take **up to 10 minutes**, and by default the code will wait for the deployment to complete.\n",
    "\n",
    "If you like, you can instead:\n",
    "\n",
    "- Un-comment the `wait=False` parameter\n",
    "- Use the [Endpoints page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) to check the status of the deployment\n",
    "- Skip over the *Evaluation* section below (which won't run until the deployment is complete), and start the Hyperparameter Optimization job - which will take a while to run too, so can be started in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time endpoint:\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.xlarge\",\n",
    "    # wait=False,  # Remember, predictor.predict() won't work until deployment finishes!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since SageMaker is a general purpose ML platform and our endpoint is a web service, we'll need to be explicit that we're sending in tabular data (_serialized_ in CSV string format for the HTTPS request) and expect a tabular response (to be _deserialized_ from CSV to numpy).\n",
    "\n",
    "In the SageMaker SDK (from v2), this packing and unpacking of the payload for the web endpoint is handled by [serializer classes](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) and [deserializer classes](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html).\n",
    "\n",
    "Unfortunately the pre-built `CSVDeserializer` produces nested Python lists of strings, rather than a numpy array of numbers - so rather than bothering to implement a custom class (like the examples [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/deserializers.py)) we'll be lazy and take this as a post-processing step.\n",
    "\n",
    "With this setup ready, requesting inferences is as easy as calling `predictor.predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.CSVDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numpy = test_data.drop([\"y\"], axis=1).values\n",
    "\n",
    "predictions = np.array(predictor.predict(X_test_numpy), dtype=float).squeeze()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in this vector is the **predicted probability** (in the interval [0,1]) of a potential customer enrolling for a term deposit.\n",
    "\n",
    "- 0: The person **will not** enroll\n",
    "- 1: The person **will** enroll (making them a good candidate for direct marketing)\n",
    "\n",
    "If we like, we could stitch these predictions back on to the original dataframe to explore performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.concat(\n",
    "    [\n",
    "        pd.Series(predictions, name=\"y_pred\", index=test_data.index),\n",
    "        test_data,\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "test_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Or use this function we provided to generate a more **comprehensive model report**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_classification_report(\n",
    "    y_actual=test_data[\"y\"].values, \n",
    "    y_predict_proba=predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Did not enroll\",\"Enrolled\"],\n",
    "    model_info=\"XGBoost SageMaker inbuilt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handling large data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have a large dataset that you would like to use for predictions. Lets create this large dataset by concatenating the training set many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data = pd.concat([test_data for i in range(100)])  # concatenate the test set 100 times to form a larger dataset\n",
    "large_data = large_data.drop([\"y\"], axis=1)  # discard the ground truth (for inference later)\n",
    "large_data.to_csv(\"data/large.csv\", index=False, header=False)  # save to a csv without headers\n",
    "\n",
    "large_uri = sgmk_session.upload_data(  # upload it to S3\n",
    "    path=\"data/large.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_large_numpy = large_data.values\n",
    "predictions = np.array(predictor.predict(X_large_numpy), dtype=float).squeeze()  # this will create an error. It is intended to do so. \n",
    "\n",
    "# Check the following cell for an explanation on why this error occurs and how you can address it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened? Why did we get this error? \n",
    "\n",
    "- **Short answer**: we sent too much data to our endpoint. \n",
    "\n",
    "- **Longer answer**: Payloads for invoking SageMaker endpoints are limited to 5MB. We sent 100 times the size of our testing set (412K rows!), which exceeded this. The endpoint couldn't handle a payload greater than 5MB.\n",
    "\n",
    "If we limit the size of data to less than 5MB, then our endpoint should be able to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_large_numpy = large_data.iloc[:10,:].values  # sent only the first 10 rows, not the entire large dataset\n",
    "predictions = np.array(predictor.predict(X_large_numpy), dtype=float).squeeze() \n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time our endpoint was able to handle the smaller size of data.\n",
    "\n",
    "This makes sense because endpoints are designed for **online (real-time) inference**. This essentially means they are optimized for **continuous arrival of small data packages**, not big chunks of data. \n",
    "\n",
    "If you want to run an one-time offline inference on a large dataset, then using Batch Transform makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Batch Transform\n",
    "\n",
    "Batch transform is optimized for **offline bulk inference of many predictions**. It is better suited for **periodic arrival** of big chunks of data (as opposed to continuous arrival of small data packages in endpoints). You can think of it as a **transient computing cluster** for an one-time inference. Once inference is finished, infrastructure is decommissioned, as opposed to endpoints which remain in service until you take them down. \n",
    "\n",
    "Let's execute an one-time inference on the large dataset using Batch Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    strategy='MultiRecord',  # or 'SingleRecord'. If SingleRecord, one line is sent at a time. if MultiRecord, more lines are sent, according to the max_payload\n",
    "    max_payload=6,  # the max payload in MB to be used. 6 is the default. \n",
    "    assemble_with='Line',\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Batch Transform may take a few minutes, depending on the dataset, the complexity of the model and the number of instances you use. \n",
    "\n",
    "More details on the arguments can be found here. \n",
    "- https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html\n",
    "- https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html?highlight=transformer#sagemaker.estimator.Estimator.transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    large_uri, \n",
    "    content_type='text/csv', \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and open the output of the Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().download_data( \n",
    "    path='data/', # destination \n",
    "    bucket=bucket_name, \n",
    "    key_prefix=f'{bucket_prefix}/large.csv.out' # source\n",
    ")\n",
    "\n",
    "batch_out = pd.read_csv('data/large.csv.out')\n",
    "batch_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameter Optimization (HPO)\n",
    "*Note, with the default settings below, the hyperparameter tuning job can take up to ~20 minutes to complete.*\n",
    "\n",
    "We will use SageMaker HyperParameter Optimization (HPO) to automate the searching process effectively. Specifically, we **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune.\n",
    "\n",
    "SageMaker hyperparameter tuning will automatically launch **multiple training jobs** with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will specify the maximum number of HPO tries (`max_jobs`) and how many of these can happen in parallel (`max_parallel_jobs`).\n",
    "\n",
    "Tip: `max_parallel_jobs` creates a **trade-off between performance and speed** (better hyperparameter values vs how long it takes to find these values). If `max_parallel_jobs` is large, then HPO is faster, but the discovered values may not be optimal. Smaller `max_parallel_jobs` will increase the chance of finding optimal values, but HPO will take more time to finish.\n",
    "\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: **validation:auc** and **train:auc**, and we elected to monitor *validation:auc* as you can see below. In this case (because it's pre-built for us), we only need to specify the metric name.\n",
    "\n",
    "For more information on the documentation of the Sagemaker HPO please refer [here](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required HPO objects\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    \"num_round\": IntegerParameter(1, 300),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 5),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:auc\"\n",
    "\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,              # the SageMaker estimator object\n",
    "    hyperparameter_ranges=ranges,     # the range of hyperparameters\n",
    "    max_jobs=20,                      # total number of HPO jobs\n",
    "    max_parallel_jobs=5,              # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",              # the internal optimization strategy of HPO\n",
    "    objective_metric_name=objective,  # the objective metric to be used for HPO\n",
    "    objective_type=\"Maximize\",        # maximize or minimize the objective metric\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch HPO\n",
    "Now we can launch a hyperparameter tuning job by calling *fit()* function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start HPO\n",
    "tuner.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO jobs often take quite a long time to finish and as such, sometimes you may want to free up the notebook and then resume the wait later.\n",
    "\n",
    "Just like the Estimator, we won't be able to `deploy()` the model until the HPO tuning job is complete; and the status is visible through both the [AWS Console](https://console.aws.amazon.com/sagemaker/home?#/hyper-tuning-jobs) and the [SageMaker API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html). We could for example write a polling script like the below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and test optimized model\n",
    "Deploying the best model is another simple `.deploy()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deploy the best model from HPO\n",
    "hpo_predictor = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.CSVDeserializer(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed, we can now evaluate the performance of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predicted probabilities of the best model\n",
    "hpo_predictions = np.array(hpo_predictor.predict(X_test_numpy), dtype=float).squeeze()\n",
    "print(hpo_predictions)\n",
    "\n",
    "# generate report for the best model\n",
    "generate_classification_report(\n",
    "    y_actual=test_data[\"y\"].values, \n",
    "    y_predict_proba=hpo_predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Did not enroll\",\"Enrolled\"],\n",
    "    model_info=\"Best model (with HPO)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "In our run, the optimized HPO model exhibited an AUC of ~0.774: fairly higher than our first-guess parameter combination!\n",
    "\n",
    "Depending on the number of tries, HPO can find a better performing model faster, compared to simply trying different hyperparameters by trial and error or grid search. You can learn more in-depth details about SageMaker HPO [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html).\n",
    "\n",
    "SageMaker built-in algorithms are great for getting a first model fast, and combining them with SageMaker HPO can really boost their accuracy.\n",
    "\n",
    "As we mentioned here, the best way to success with a built-in algorithm is to **read the [algorithm's doc pages](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) carefully** - to understand what data format and parameters it needs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Releasing Cloud Resources\n",
    "\n",
    "It's generally a good practice to deactivate all endpoints which are not in use.  \n",
    "\n",
    "Please uncomment the following lines and run the cell in order to deactivate the 2 endpoints that were created before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hpo_predictor.delete_endpoint(delete_endpoint_config=True)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
