{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing with Amazon SageMaker\n",
    "\n",
    "In production ML workflows, data scientists and data engineers frequently try to improve their models in various ways, such as by performing [Perform Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html), training on additional or more-recent data, and improving feature selection. Performing A/B testing between a new model and an old model with production traffic can be an effective final step in the validation process for a new model. In A/B testing, you test different variants of your models and compare how each variant performs relative to each other. You then choose the best-performing model to replace a previously-existing model new version delivers better performance than the previously-existing version.\n",
    "\n",
    "Amazon SageMaker enables you to test multiple models or model versions behind the same endpoint using production variants. Each production variant identifies a machine learning (ML) model and the resources deployed for hosting the model. You can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant, or you can invoke a specific variant directly for each request.\n",
    "\n",
    "In this notebook we'll:\n",
    "* Evaluate models by invoking specific variants\n",
    "* Gradually release a new model by specifying traffic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerrequisites\n",
    "\n",
    "First we ensure we have an updated version of boto3, which includes the latest SageMaker features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up some required imports and basic initial variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "region= boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_session = session.Session(boto3.Session())\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "# You can use a different bucket, but make sure the role you chose for this notebook\n",
    "# has the s3:PutObject permissions. This is the bucket into which the model artifacts will be uploaded\n",
    "bucket =  sm_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-VariantTargeting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create and deploy the models\n",
    "\n",
    "### First, we upload our pre-trained models to Amazon S3\n",
    "This code uploads two pre-trained XGBoost models that are ready for you to deploy. These models were trained using the XGB Churn Prediction Notebook in SageMaker. You can also use your own pre-trained models in this step. If you already have a pretrained model in Amazon S3, you can add it by specifying the s3_key.\n",
    "\n",
    "The models in this example are used to predict the probability of a mobile customer leaving their current mobile operator. The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = S3Uploader.upload(local_path=\"model/xgb-churn-prediction-model.tar.gz\",\n",
    "                              desired_s3_uri=f\"s3://{bucket}/{prefix}\")\n",
    "model_url2 = S3Uploader.upload(local_path=\"model/xgb-churn-prediction-model2.tar.gz\",\n",
    "                              desired_s3_uri=f\"s3://{bucket}/{prefix}\")\n",
    "model_url, model_url2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create our model definitions\n",
    "Start with deploying the pre-trained churn prediction models. Here, you create the model objects with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "model_name = f\"DEMO-xgb-churn-pred-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "model_name2 = f\"DEMO-xgb-churn-pred2-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "image_uri = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "image_uri2 = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-2')\n",
    "\n",
    "sm_session.create_model(name=model_name, role=role, container_defs={\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': model_url\n",
    "})\n",
    "\n",
    "sm_session.create_model(name=model_name2, role=role, container_defs={\n",
    "    'Image': image_uri2,\n",
    "    'ModelDataUrl': model_url2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variants\n",
    "\n",
    "We now create two variants, each with its own different model (these could also have different instance types and counts).\n",
    "\n",
    "We set an initial_weight of “1” for both variants: this means 50% of our requests go to Variant1, and the remaining 50% of all requests to Variant2. (The sum of weights across both variants is 2 and each variant has weight assignment of 1. This implies each variant receives 1/2, or 50%, of the total traffic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "variant1 = production_variant(model_name=model_name,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              initial_instance_count=1,\n",
    "                              variant_name='Variant1',\n",
    "                              initial_weight=1)\n",
    "variant2 = production_variant(model_name=model_name2,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              initial_instance_count=1,\n",
    "                              variant_name='Variant2',\n",
    "                              initial_weight=1)\n",
    "\n",
    "(variant1, variant2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy\n",
    "\n",
    "Let's go ahead and deploy our two variants to a SageMaker endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"DEMO-xgb-churn-pred-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(f\"EndpointName={endpoint_name}\")\n",
    "\n",
    "sm_session.endpoint_from_production_variants(\n",
    "    name=endpoint_name,\n",
    "    production_variants=[variant1, variant2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Invoke the deployed models\n",
    "\n",
    "You can now send data to this endpoint to get inferences in real time.\n",
    "\n",
    "This step invokes the endpoint with included sample data for about 2 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of test data for a quick test\n",
    "!tail -120 test_data/test-dataset-input-cols.csv > test_data/test_sample_tail_input_cols.csv\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "\n",
    "with open('test_data/test_sample_tail_input_cols.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "print(\"Done!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocations per variant\n",
    "\n",
    "Amazon SageMaker emits metrics such as Latency and Invocations (full list of metrics [here](https://alpha-docs-aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)) for each variant in Amazon CloudWatch. Let’s query CloudWatch to get number of Invocations per variant, to show how invocations are split across variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cw = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "def get_invocation_metrics_for_endpoint_variant(endpoint_name,\n",
    "                                                variant_name,\n",
    "                                                start_time,\n",
    "                                                end_time):\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=\"Invocations\",\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60,\n",
    "        Statistics=[\"Sum\"],\n",
    "        Dimensions=[\n",
    "            {\n",
    "                \"Name\": \"EndpointName\",\n",
    "                \"Value\": endpoint_name\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"VariantName\",\n",
    "                \"Value\": variant_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return pd.DataFrame(metrics[\"Datapoints\"])\\\n",
    "            .sort_values(\"Timestamp\")\\\n",
    "            .set_index(\"Timestamp\")\\\n",
    "            .drop(\"Unit\", axis=1)\\\n",
    "            .rename(columns={\"Sum\": variant_name})\n",
    "\n",
    "def plot_endpoint_metrics(start_time=None):\n",
    "    start_time = start_time or datetime.now() - timedelta(minutes=60)\n",
    "    end_time = datetime.now()\n",
    "    metrics_variant1 = get_invocation_metrics_for_endpoint_variant(endpoint_name, variant1[\"VariantName\"], start_time, end_time)\n",
    "    metrics_variant2 = get_invocation_metrics_for_endpoint_variant(endpoint_name, variant2[\"VariantName\"], start_time, end_time)\n",
    "    metrics_variants = metrics_variant1.join(metrics_variant2, how=\"outer\")\n",
    "    metrics_variants.plot()\n",
    "    return metrics_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting a minute for initial metric creation...\")\n",
    "time.sleep(60)\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke a specific variant\n",
    "\n",
    "Now, let’s use the new feature that was released today to invoke a specific variant. For this, we simply use the new parameter to define which specific ProductionVariant we want to invoke. Let us use this to invoke Variant1 for all requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = ''\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "with open('test_data/test_sample_tail_input_cols.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload,\n",
    "                                   TargetVariant=variant1[\"VariantName\"])\n",
    "        predictions = ','.join([predictions, response['Body'].read().decode('utf-8')])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert our predictions to a numpy array\n",
    "pred_np = np.fromstring(predictions[1:], sep=',')\n",
    "        \n",
    "# Convert the prediction probabilities to binary predictions of either 1 or 0\n",
    "threshold = 0.5\n",
    "preds = np.where(pred_np > threshold, 1, 0)    \n",
    "print(\"Done!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we again check the traffic per variant, this time we see that the number of invocations only incremented for Variant1, because all invocations were targeted at that variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20) #let metrics catch up\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate variant performance\n",
    "\n",
    "### Evaluating Variant 1\n",
    "\n",
    "Using the new targeting feature, let us evaluate the accuracy, precision, recall, F1 score, and ROC/AUC for Variant1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Let's get the labels of our test set; we will use these to evaluate our predictions\n",
    "!tail -121 test_data/test-dataset.csv > test_data/test_dataset_sample_tail.csv\n",
    "df_with_labels = pd.read_csv('test_data/test_dataset_sample_tail.csv')\n",
    "test_labels = df_with_labels.iloc[:, 0]\n",
    "labels = test_labels.to_numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(preds == labels) / len(labels)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate precision\n",
    "precision = sum(preds[preds == 1] == labels[preds == 1]) / len(preds[preds == 1])\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = sum(preds[preds == 1] == labels[preds == 1]) / len(labels[labels == 1])\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1 Score: {f1_score}')\n",
    "\n",
    "# Calculate AUC\n",
    "auc = round(roc_auc_score(labels, preds), 4)\n",
    "print('AUC is ' + repr(auc))\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b',\n",
    "label='AUC = %0.2f'% auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we collect data for Variant2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = ''\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "with open('test_data/test_sample_tail_input_cols.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload,\n",
    "                                   TargetVariant=variant2[\"VariantName\"])\n",
    "        predictions2 = ','.join([predictions2, response['Body'].read().decode('utf-8')])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert to numpy array        \n",
    "pred_np2 = np.fromstring(predictions2[1:], sep=',')\n",
    "        \n",
    "# Convert to binary predictions\n",
    "thresh = 0.5\n",
    "preds2 = np.where(pred_np2 > threshold, 1, 0)    \n",
    "\n",
    "print(\"Done!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we again check the traffic per variant, this time we see that the number of invocations only incremented for Variant2, because all invocations were targeted at that variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60) # give metrics time to catch up\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Variant2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy2 = sum(preds2 == labels) / len(labels)\n",
    "print(f'Accuracy: {accuracy2}')\n",
    "\n",
    "# Calculate precision\n",
    "precision2 = sum(preds2[preds2 == 1] == labels[preds2 == 1]) / len(preds2[preds2 == 1])\n",
    "print(f'Precision: {precision2}')\n",
    "\n",
    "# Calculate recall\n",
    "recall2 = sum(preds2[preds2 == 1] == labels[preds2 == 1]) / len(labels[labels == 1])\n",
    "print(f'Recall: {recall2}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score2 = 2 * (precision2 * recall2) / (precision2 + recall2)\n",
    "print(f'F1 Score: {f1_score2}')\n",
    "\n",
    "auc2 = round(roc_auc_score(labels, preds2), 4)\n",
    "print('AUC is ' + repr(auc2))\n",
    "\n",
    "fpr2, tpr2, _ = metrics.roc_curve(labels, preds2)\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr2, tpr2, 'b',\n",
    "label='AUC = %0.2f'% auc2)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Variant2 is performing better for most of our defined metrics, so this is the one we’re likely to choose to dial up in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Dialing up our chosen variant in production\n",
    "\n",
    "Now that we have determined Variant2 to be better as compared to Variant1, we will shift more traffic to it. \n",
    "\n",
    "We can continue to use TargetVariant to continue invoking a chosen variant. A simpler approach is to update the weights assigned to each variant using UpdateEndpointWeightsAndCapacities. This changes the traffic distribution to your production variants without requiring updates to your endpoint. \n",
    "\n",
    "Recall our variant weights are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sm.describe_endpoint(EndpointName=endpoint_name)[\"ProductionVariants\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first write a method to easily invoke our endpoint (a copy of what we had been previously doing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint_for_two_minutes():\n",
    "    with open('test_data/test-dataset-input-cols.csv', 'r') as f:\n",
    "        for row in f:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  Body=payload)\n",
    "            response['Body'].read()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke our endpoint for a bit, to show the even split in invocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_start_time = datetime.now()\n",
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(20) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us shift 75% of the traffic to Variant2 by assigning new weights to each variant using UpdateEndpointWeightsAndCapacities. Amazon SageMaker will now send 75% of the inference requests to Variant2 and remaining 25% of requests to Variant1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            \"DesiredWeight\": 25,\n",
    "            \"VariantName\": variant1[\"VariantName\"]\n",
    "        },\n",
    "        {\n",
    "            \"DesiredWeight\": 75,\n",
    "            \"VariantName\": variant2[\"VariantName\"]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for update to complete\")\n",
    "while True:\n",
    "    status = sm.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        print(\"Done\")\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sm.describe_endpoint(EndpointName=endpoint_name)[\"ProductionVariants\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how that has impacted invocation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(20) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to monitor our metrics and when we're satisfied with a variant's performance, we can route 100% of the traffic over the variant. We used UpdateEndpointWeightsAndCapacities to update the traffic assignments for the variants. The weight for Variant1 is set to 0 and the weight for Variant2 is set to 1. Therefore, Amazon SageMaker will send 100% of all inference requests to Variant2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            \"DesiredWeight\": 0,\n",
    "            \"VariantName\": variant1[\"VariantName\"]\n",
    "        },\n",
    "        {\n",
    "            \"DesiredWeight\": 1,\n",
    "            \"VariantName\": variant2[\"VariantName\"]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Waiting for update to complete\")\n",
    "while True:\n",
    "    status = sm.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        print(\"Done\")\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sm.describe_endpoint(EndpointName=endpoint_name)[\"ProductionVariants\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(20) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon CloudWatch metrics for the total invocations for each variant below shows us that all inference requests are being processed by Variant2 and there are no inference requests processed by Variant1.\n",
    "\n",
    "You can now safely update your endpoint and delete Variant1 from your endpoint. You can also continue testing new models in production by adding new variants to your endpoint and following steps 2 - 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n",
    "\n",
    "If you do not plan to use this endpoint further, you should delete the endpoint to avoid incurring additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
